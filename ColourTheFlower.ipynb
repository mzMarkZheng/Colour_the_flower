{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnYtM0C-KxkG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.utils import save_image\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "from fastai.vision.all import untar_data, URLs\n",
        "import sys\n",
        "\n",
        "from torch.utils.data import (\n",
        "    DataLoader,\n",
        ")  # Gives easier dataset managment and creates mini batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK9KtMhCLUW5"
      },
      "outputs": [],
      "source": [
        "class FlowerDataset(Dataset):\n",
        "    def __init__(self, dir):\n",
        "        self.dir = dir\n",
        "        self.list_of_files = [dir for dir in os.listdir(self.dir) if dir != \".ipynb_checkpoints\"] # for some reason colab created this folder inside train and/or test\n",
        "\n",
        "       \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_of_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get actual file path\n",
        "        filename = self.list_of_files[index]\n",
        "        path = os.path.join(self.dir, filename)\n",
        "\n",
        "        # load initial image with both coloured and gray image.\n",
        "        image = Image.open(path)\n",
        "\n",
        "        # Get the coloured image by converting it to a numpy array and subsetting.\n",
        "        image_colour = np.array(image)[:, :512, :]\n",
        "\n",
        "        #print('converting to tensor')\n",
        "        to_tensor = transforms.ToTensor()\n",
        "        image_colour = to_tensor(image_colour)\n",
        "\n",
        "        #print('greyscaling bw image')\n",
        "        bw_transform = transforms.Grayscale()\n",
        "\n",
        "        image_bw = bw_transform(image_colour)\n",
        "\n",
        "        normalise = transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])\n",
        "        image_colour = normalise(image_colour)\n",
        "\n",
        "        #print('Done')\n",
        "        return (image_bw, image_colour)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L17hcpF_PByK"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 2e-4\n",
        "TRAIN_DIR = \"Data/train\"\n",
        "TEST_DIR = \"Data/test\"\n",
        "# TRAIN_DIR = \"quick_data/train\"\n",
        "# TEST_DIR = \"quick_data/test\"\n",
        "EXAMPLE_DIR = \"Example_results\"\n",
        "\n",
        "# The paper uses a batch size of 1\n",
        "BATCH_SIZE = 1\n",
        "NUM_WORKERS = 0\n",
        "IMAGE_SIZE = 512\n",
        "CHANNELS_IMG = 3\n",
        "NUM_EPOCHS = 500\n",
        "L1_LAMBDA = 100\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = True\n",
        "SAVE_MODEL_EVERY_NTH = 5\n",
        "CHECKPOINT_DISC = \"Checkpoint/discriminator.pth.tar\"\n",
        "CHECKPOINT_GEN = \"Checkpoint/generator.pth.tar\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3g1P8pMPKWS"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.initial = nn.Sequential(\n",
        "            # Here in_channels is multiplied by two because we're going to send in both images concatenated in the channel dimension.\n",
        "            # aka we'll be having 6 channels. (3 for each image)\n",
        "            nn.Conv2d(in_channels*2, features[0], kernel_size=4,\n",
        "                      stride=2, padding=1, padding_mode=\"reflect\"),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        in_channels = features[0]\n",
        "\n",
        "        for feature in features[1:]:\n",
        "            layers.append(\n",
        "                downsampling_conv(in_channels, feature,\n",
        "                                  stride=1 if feature == features[-1] else 2)\n",
        "            )\n",
        "            in_channels = feature\n",
        "\n",
        "        layers.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat([x, y], dim=1)\n",
        "        x = self.initial(x)\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Convolution segment for the discriminator(downsampling)\n",
        "# Convolution -> BatchNorm -> leakyRelu\n",
        "class downsampling_conv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, ks=4, stride=2, pad=1, bn=True):\n",
        "        super().__init__()\n",
        "        # stores if the batch norm should be applied\n",
        "        self.bn = bn\n",
        "\n",
        "        self.conv_downsample = nn.Conv2d(\n",
        "            in_planes, out_planes, ks, stride, pad, padding_mode=\"reflect\")\n",
        "        self.batch_norm = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        LEAKY_RELU_SLOPE = 0.2\n",
        "        self.leaky_relu = nn.LeakyReLU(LEAKY_RELU_SLOPE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Run through convolution then BatchNorm then leaky ReLU\n",
        "\n",
        "        x = self.conv_downsample(x)\n",
        "\n",
        "        if self.bn:\n",
        "            x = self.batch_norm(x)\n",
        "\n",
        "        x = self.leaky_relu(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttmb2hy-POD9"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # input:\n",
        "        self.input_conv = nn.Conv2d(\n",
        "            in_channels, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "        # encoder:\n",
        "        # C64-C128-C256-C512-C512-C512-C512-C512\n",
        "        self.enc_c64_128 = enc_downsampling_conv(64, 128, bn=False)\n",
        "        self.enc_c128_256 = enc_downsampling_conv(128, 256)\n",
        "        self.enc_c256_512 = enc_downsampling_conv(256, 512)\n",
        "        self.enc_c512_512 = enc_downsampling_conv(512, 512)\n",
        "\n",
        "        # decoder\n",
        "        # CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
        "        # Note this is the U-Net version\n",
        "        self.dec_cd512_512 = dec_dropout_upsampling_conv(512, 512)\n",
        "        self.dec_cd1024_512 = dec_dropout_upsampling_conv(1024, 512)\n",
        "\n",
        "        self.dec_c1024_512 = dec_upsampling_conv(1024, 512)\n",
        "        self.dec_c1024_256 = dec_dropout_upsampling_conv(1024, 256)\n",
        "        self.dec_c512_128 = dec_upsampling_conv(512, 128)\n",
        "        self.dec_c256_64 = dec_upsampling_conv(256, 64)\n",
        "\n",
        "        # output\n",
        "        self.output_conv = nn.ConvTranspose2d(\n",
        "            128, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, out):\n",
        "\n",
        "        # INPUT\n",
        "        out = self.input_conv(out)\n",
        "        out = self.leaky_relu(out)\n",
        "        skip_l1 = out\n",
        "\n",
        "        # ENCODE\n",
        "        # down 1:\n",
        "        out = self.enc_c64_128(out)\n",
        "        skip_l2 = out\n",
        "        # down 2:\n",
        "        out = self.enc_c128_256(out)\n",
        "        skip_l3 = out\n",
        "        # down 3:\n",
        "        out = self.enc_c256_512(out)\n",
        "        skip_l4 = out\n",
        "        # down 4:\n",
        "        out = self.enc_c512_512(out)\n",
        "        skip_l5 = out\n",
        "        # down 5:\n",
        "        out = self.enc_c512_512(out)\n",
        "        skip_l6 = out\n",
        "        # down 6:\n",
        "        out = self.enc_c512_512(out)\n",
        "        skip_l7 = out\n",
        "\n",
        "        # bottleneck:\n",
        "        out = self.enc_c512_512(out)\n",
        "\n",
        "        # DECODE\n",
        "        # up 1:\n",
        "        out = self.dec_cd512_512(out)\n",
        "        # up 2:\n",
        "        out = torch.cat((out, skip_l7), 1)\n",
        "        out = self.dec_cd1024_512(out)\n",
        "        # up 3:\n",
        "        out = torch.cat((out, skip_l6), 1)\n",
        "        out = self.dec_cd1024_512(out)\n",
        "        # up 4:\n",
        "        out = torch.cat((out, skip_l5), 1)\n",
        "        out = self.dec_c1024_512(out)\n",
        "        # up 5:\n",
        "        out = torch.cat((out, skip_l4), 1)\n",
        "        out = self.dec_c1024_256(out)\n",
        "        # up 6:\n",
        "        out = torch.cat((out, skip_l3), 1)\n",
        "        out = self.dec_c512_128(out)\n",
        "        # up 7:\n",
        "        out = torch.cat((out, skip_l2), 1)\n",
        "        out = self.dec_c256_64(out)\n",
        "\n",
        "        # OUTPUT\n",
        "        out = torch.cat((out, skip_l1), 1)\n",
        "        out = self.output_conv(out)\n",
        "        out = self.tanh(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Convolution segment for encoding (downsampling)\n",
        "# Convolution -> BatchNorm -> leakyRelu\n",
        "class enc_downsampling_conv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, ks=4, stride=2, pad=1, bn=True):\n",
        "        super().__init__()\n",
        "        # stores if the batch norm should be applied\n",
        "        self.bn = bn\n",
        "\n",
        "        self.conv_downsample = nn.Conv2d(\n",
        "            in_planes, out_planes, ks, stride, pad, padding_mode=\"reflect\")\n",
        "        self.batch_norm = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        LEAKY_RELU_SLOPE = 0.2\n",
        "        self.leaky_relu = nn.LeakyReLU(LEAKY_RELU_SLOPE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Run through convolution then BatchNorm then leaky ReLU\n",
        "\n",
        "        x = self.conv_downsample(x)\n",
        "\n",
        "        if self.bn:\n",
        "            x = self.batch_norm(x)\n",
        "\n",
        "        x = self.leaky_relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Convolution segment for decoding (upsampling) with dropout\n",
        "# Convolution -> BatchNorm -> Dropout -> Relu\n",
        "\n",
        "class dec_dropout_upsampling_conv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, ks=4, stride=2, pad=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_upsample = nn.ConvTranspose2d(\n",
        "            in_planes, out_planes, ks, stride, pad)\n",
        "\n",
        "        self.batch_norm = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        DROPOUT_RATE = 0.5\n",
        "        self.dropout = nn.Dropout2d(DROPOUT_RATE)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Run through convolution then dropout then ReLU\n",
        "        x = self.conv_upsample(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Convolution segment for decoding (upsampling) without dropout\n",
        "# Convolution -> BatchNorm -> Relu\n",
        "\n",
        "\n",
        "class dec_upsampling_conv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, ks=4, stride=2, pad=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_upsample = nn.ConvTranspose2d(\n",
        "            in_planes, out_planes, ks, stride, pad)\n",
        "\n",
        "        self.batch_norm = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Run through convolution then BatchNorm then ReLU\n",
        "\n",
        "        x = self.conv_upsample(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAL_m2d4PP6R"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def save_some_examples(gen, val_loader, epoch, folder, num_photos=12):\n",
        "    x, y = next(iter(val_loader))\n",
        "    x, y = x.to(DEVICE, dtype=torch.float), y.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        index = 0\n",
        "        label_tensor = None\n",
        "        output_tensor = None\n",
        "        for data in itertools.islice(iter(val_loader), num_photos):\n",
        "          x, y = data\n",
        "          x, y = x.to(DEVICE, dtype=torch.float), y.to(DEVICE, dtype=torch.float)\n",
        "\n",
        "          y_fake = gen(x)\n",
        "\n",
        "          # remove normalization#\n",
        "          y = y * 0.5 + 0.5\n",
        "          y_fake = y_fake * 0.5 + 0.5\n",
        "\n",
        "          if label_tensor is None:\n",
        "            label_tensor = torch.Tensor(y)\n",
        "          else:\n",
        "            label_tensor = torch.cat((label_tensor, y), 3) # The 3 makes the concatenation happen column-wise.\n",
        "\n",
        "          y_fake_denormalized = y_fake\n",
        "          \n",
        "\n",
        "          if output_tensor is None:\n",
        "            output_tensor = torch.Tensor(y_fake)\n",
        "          else:\n",
        "            output_tensor = torch.cat((output_tensor, y_fake), 3)      \n",
        "            \n",
        "\n",
        "        save_image(torch.cat((output_tensor, label_tensor), 2), folder + f\"/epoch{epoch}.png\")\n",
        "    gen.train()\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqzfASqFPTul"
      },
      "outputs": [],
      "source": [
        "def train_func(disc, gen, loader, optim_disc, optim_gen, l1_loss, bce, g_scaler, d_scaler, epoch_no\n",
        "               ):\n",
        "\n",
        "    # tqdm is for progress bar\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for idx, (x, y) in enumerate(loop):\n",
        "        \n",
        "        # x is bw image, y is colour image\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        # Many functions only work with float data type\n",
        "        x = x.float()\n",
        "        y = y.float()\n",
        "\n",
        "        # x3 = torch.cat((x, x, x), 1)\n",
        "\n",
        "        #print(x.size())\n",
        "        #print(y.size())\n",
        "\n",
        "        # Train the discriminator\n",
        "        with torch.cuda.amp.autocast():\n",
        "            \n",
        "            y_fake = gen(x)\n",
        "            \n",
        "            # Loss from fake image\n",
        "            D_fake = disc(y, y_fake.detach())\n",
        "            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n",
        "\n",
        "            # Loss from real image\n",
        "            D_real = disc(y, y)\n",
        "            D_real_loss = bce(D_real, torch.ones_like(D_real))\n",
        "\n",
        "            # Some sources says that the discriminator trains too fast compared to the generator, so it is halved\n",
        "            # Another source did not have it halved\n",
        "            D_loss = (D_real_loss + D_fake_loss) / 2\n",
        "\n",
        "        if torch.sigmoid(D_fake).mean().item() > 0.35:\n",
        "          disc.zero_grad()\n",
        "          d_scaler.scale(D_loss).backward()\n",
        "          d_scaler.step(optim_disc)\n",
        "          d_scaler.update()\n",
        "\n",
        "        # Train the generator\n",
        "        with torch.cuda.amp.autocast():\n",
        "            D_fake = disc(y, y_fake)\n",
        "            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
        "\n",
        "            # L1 is a loss function (least absolute deviations)\n",
        "            L1 = l1_loss(y_fake, y) * L1_LAMBDA\n",
        "            G_loss = G_fake_loss + L1\n",
        "\n",
        "\n",
        "        optim_gen.zero_grad()\n",
        "        g_scaler.scale(G_loss).backward()\n",
        "\n",
        "        g_scaler.step(optim_gen)\n",
        "        g_scaler.update()\n",
        "\n",
        "\n",
        "        if idx == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch_no}/{NUM_EPOCHS}] Batch {idx}/{len(loader)} \\\n",
        "                      Loss D: {D_loss:.4f}, loss G: {G_loss:.4f}\"\n",
        "            )\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            loop.set_postfix(\n",
        "                D_real=torch.sigmoid(D_real).mean().item(),\n",
        "                D_fake=torch.sigmoid(D_fake).mean().item(),\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    disc = Discriminator(in_channels=3).to(DEVICE)\n",
        "    gen = Generator(in_channels=1, out_channels=3).to(DEVICE)\n",
        "\n",
        "    # Standard values for Adam beta 1 is 0.9, but paper has 0.5\n",
        "\n",
        "    # Optimiser for the discriminator and generator\n",
        "    optim_disc = optim.Adam(\n",
        "        disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    optim_gen = optim.Adam(\n",
        "        gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "    # Loss\n",
        "    BCE = nn.BCEWithLogitsLoss()\n",
        "    L1_LOSS = nn.L1Loss()\n",
        "\n",
        "    # Load model if we have LOAD_MODEL set to True in configs\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(CHECKPOINT_DISC, disc,\n",
        "                        optim_disc, LEARNING_RATE)\n",
        "        load_checkpoint(CHECKPOINT_GEN, gen, optim_gen,\n",
        "                        LEARNING_RATE)\n",
        "\n",
        "\n",
        "    #transform = transforms.Compose([transforms.ToTensor(),\n",
        "     #                                     transforms.Normalize([0.485, 0.456, 0.406], \n",
        "      #                                                         [0.229, 0.224, 0.225])])\n",
        "    # Load training dataset\n",
        "    train_dataset = FlowerDataset(TRAIN_DIR)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # float16 training\n",
        "    g_scaler = torch.cuda.amp.GradScaler()\n",
        "    d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # Load the testing/validation dataset\n",
        "    test_dataset = FlowerDataset(TEST_DIR)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "    \n",
        "\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f'====\\nEpoch {epoch}\\n====')\n",
        "\n",
        "        \n",
        "        train_func(\n",
        "            disc, gen, train_loader, optim_disc, optim_gen, L1_LOSS, BCE, g_scaler, d_scaler, epoch,\n",
        "        )\n",
        "\n",
        "        if SAVE_MODEL and epoch % SAVE_MODEL_EVERY_NTH == 0:\n",
        "            save_checkpoint(gen, optim_gen, filename=CHECKPOINT_GEN)\n",
        "            save_checkpoint(disc, optim_disc, filename=CHECKPOINT_DISC)\n",
        "\n",
        "        save_some_examples(gen, test_loader, epoch, folder=EXAMPLE_DIR)\n",
        "\n",
        "    # If by chance epoch number is not divisible, we save the very last epoch\n",
        "    if SAVE_MODEL and epoch % SAVE_MODEL_EVERY_NTH != 0:\n",
        "        save_checkpoint(gen, optim_gen, filename=CHECKPOINT_GEN)\n",
        "        save_checkpoint(disc, optim_disc, filename=CHECKPOINT_DISC)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "hDOKhSL3QsBd",
        "outputId": "5e7e46f3-1fa7-45a1-f4b4-f32b3e41696e"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2df0e9bfb44adb10e319a9e96d9003be3bb516579347c0c7a0c6e1e326021008"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
